{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "SSAbb8lKLoXd"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import TrainingArguments, BitsAndBytesConfig, pipeline\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "from trl import DPOTrainer, DPOConfig\n",
    "import torch\n",
    "import huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_hub.login(token='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')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5520,
     "status": "ok",
     "timestamp": 1767016886094,
     "user": {
      "displayName": "Mojtaba Alihosseini",
      "userId": "12922012540103251507"
     },
     "user_tz": -210
    },
    "id": "5z2XYv5q1xTW",
    "outputId": "b897749d-5fe3-4ae9-a091-89ef16171468"
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"Intel/orca_dpo_pairs\")\n",
    "dataset = dataset['train']\n",
    "dataset = dataset.shuffle(seed=42).select(range(1000))\n",
    "split = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "train_dataset = split[\"train\"]\n",
    "eval_dataset  = split[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['system', 'question', 'chosen', 'rejected'],\n",
       "    num_rows: 800\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['system', 'question', 'chosen', 'rejected'],\n",
       "    num_rows: 200\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "oLOAkWwNP-0m"
   },
   "outputs": [],
   "source": [
    "def chatml_format(example):\n",
    "  system_prompt = f\"<|im_start|>system\\n{example['system']}\\n<|im_end|>\\n\"\n",
    "  user_prompt = f\"<|im_start|>user\\n{example['question']}\\n<|im_end|>\\n\"\n",
    "  assistant_prompt = \"<|im_start|>assistant\\n\"\n",
    "  return {\n",
    "        \"prompt\": system_prompt + user_prompt + assistant_prompt,\n",
    "        \"chosen\": example['chosen'],\n",
    "        \"rejected\": example['rejected'],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "x1f0OVAUX2D3"
   },
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.map(\n",
    "    chatml_format,\n",
    "    remove_columns=dataset.column_names\n",
    ")\n",
    "\n",
    "eval_dataset = eval_dataset.map(\n",
    "    chatml_format,\n",
    "    remove_columns=dataset.column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 128,
     "status": "ok",
     "timestamp": 1767016895570,
     "user": {
      "displayName": "Mojtaba Alihosseini",
      "userId": "12922012540103251507"
     },
     "user_tz": -210
    },
    "id": "PxeQaCqzX87D",
    "outputId": "a4d80b3e-aed8-4b31-8e5b-310d32b66c1a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chosen': 'Mexico and the United States have signed an agreement to strengthen security at their shared border, according to the Mexican Public Security Ministry.',\n",
       " 'rejected': \" Sure! Here's a very short summary of the text:\\n\\nMexico and the United States have signed an agreement to strengthen security at their shared border.\",\n",
       " 'prompt': '<|im_start|>system\\nYou are an AI assistant. You will be given a task. You must generate a detailed and long answer.\\n<|im_end|>\\n<|im_start|>user\\nmexico and the united states signed an agreement thursday to reinforce the security at the common border , the mexican public security ministry said .\\n\\nWhat is a very short summary of the above text?\\n<|im_end|>\\n<|im_start|>assistant\\n'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "hO89MOeVMmDh"
   },
   "outputs": [],
   "source": [
    "model_name = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>system\\nYou are an AI assistant. You will be given a task. You must generate a detailed and long answer.\\n<|im_end|>\\n<|im_start|>user\\nmexico and the united states signed an agreement thursday to reinforce the security at the common border , the mexican public security ministry said .\\n\\nWhat is a very short summary of the above text?\\n<|im_end|>\\n<|im_start|>assistant\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[1]['prompt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rejected is : Mexico and the United States have signed an agreement to strengthen security at their shared border, according to the Mexican Public Security Ministry.\n",
      "chosen is :  Sure! Here's a very short summary of the text:\n",
      "\n",
      "Mexico and the United States have signed an agreement to strengthen security at their shared border.\n",
      "model generated output is : The U. S. and Mexico agreed on Thursday to strengthen border security, according to Mexican officials.\n",
      "This extremely concise summary captures the essence of the original statement in just 12 words, providing a quick overview of the key points without any extraneous information. The summary effectively conveys that two countries have reached an agreement regarding border safety, which appears to involve cooperation between law enforcement agencies or other relevant organizations to enhance protection along their shared borders.\n"
     ]
    }
   ],
   "source": [
    "print('rejected is :',train_dataset[1]['chosen'])\n",
    "print('chosen is :',train_dataset[1]['rejected'])\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "prompt = train_dataset[1]['prompt']\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "input_token_length = inputs[\"input_ids\"].shape[1]\n",
    "output = tokenizer.decode(\n",
    "    model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=100,\n",
    "    )[0][input_token_length:],\n",
    "    skip_special_tokens=True\n",
    ")\n",
    "print('model generated output is :', output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "b8h_b-4Faped"
   },
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=\"all-linear\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpo_config = DPOConfig(\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    gradient_checkpointing=True,\n",
    "    remove_unused_columns=False,\n",
    "    learning_rate=5.0e-06,\n",
    "    eval_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    num_train_epochs=6,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=1,\n",
    "    output_dir=\"./qwen2-dpo-orca\",\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    warmup_steps=2,\n",
    "    bf16=True,\n",
    "    report_to=\"none\",\n",
    "    beta=0.1,\n",
    "    max_prompt_length=2048,\n",
    "    max_length=2048,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = DPOTrainer(\n",
    "    model,\n",
    "    args=dpo_config,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    processing_class=tokenizer,\n",
    "    peft_config=peft_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='600' max='600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [600/600 28:42, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rewards/chosen</th>\n",
       "      <th>Rewards/rejected</th>\n",
       "      <th>Rewards/accuracies</th>\n",
       "      <th>Rewards/margins</th>\n",
       "      <th>Logps/chosen</th>\n",
       "      <th>Logps/rejected</th>\n",
       "      <th>Logits/chosen</th>\n",
       "      <th>Logits/rejected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.287700</td>\n",
       "      <td>0.128795</td>\n",
       "      <td>-1.019058</td>\n",
       "      <td>-5.474181</td>\n",
       "      <td>0.955000</td>\n",
       "      <td>4.455123</td>\n",
       "      <td>-174.196411</td>\n",
       "      <td>-286.691895</td>\n",
       "      <td>-0.842329</td>\n",
       "      <td>-0.978002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.053200</td>\n",
       "      <td>0.075451</td>\n",
       "      <td>-1.825165</td>\n",
       "      <td>-9.913341</td>\n",
       "      <td>0.970000</td>\n",
       "      <td>8.088176</td>\n",
       "      <td>-182.257462</td>\n",
       "      <td>-331.083527</td>\n",
       "      <td>-0.858395</td>\n",
       "      <td>-0.961803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.023200</td>\n",
       "      <td>0.056552</td>\n",
       "      <td>-2.118336</td>\n",
       "      <td>-11.838317</td>\n",
       "      <td>0.970000</td>\n",
       "      <td>9.719979</td>\n",
       "      <td>-185.189178</td>\n",
       "      <td>-350.333252</td>\n",
       "      <td>-0.878102</td>\n",
       "      <td>-0.978861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.012800</td>\n",
       "      <td>0.050093</td>\n",
       "      <td>-2.255131</td>\n",
       "      <td>-12.749587</td>\n",
       "      <td>0.970000</td>\n",
       "      <td>10.494455</td>\n",
       "      <td>-186.557129</td>\n",
       "      <td>-359.445923</td>\n",
       "      <td>-0.886178</td>\n",
       "      <td>-0.988147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.009500</td>\n",
       "      <td>0.048643</td>\n",
       "      <td>-2.376970</td>\n",
       "      <td>-13.282022</td>\n",
       "      <td>0.970000</td>\n",
       "      <td>10.905050</td>\n",
       "      <td>-187.775513</td>\n",
       "      <td>-364.770325</td>\n",
       "      <td>-0.904641</td>\n",
       "      <td>-1.006998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.008700</td>\n",
       "      <td>0.049035</td>\n",
       "      <td>-2.394111</td>\n",
       "      <td>-13.352375</td>\n",
       "      <td>0.970000</td>\n",
       "      <td>10.958265</td>\n",
       "      <td>-187.946915</td>\n",
       "      <td>-365.473816</td>\n",
       "      <td>-0.905259</td>\n",
       "      <td>-1.006544</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/main/lib/python3.12/site-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/venv/main/lib/python3.12/site-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/venv/main/lib/python3.12/site-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/venv/main/lib/python3.12/site-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/venv/main/lib/python3.12/site-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=600, training_loss=0.06585770606994629, metrics={'train_runtime': 1724.3484, 'train_samples_per_second': 2.784, 'train_steps_per_second': 0.348, 'total_flos': 0.0, 'train_loss': 0.06585770606994629, 'epoch': 6.0})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    }
   ],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_MODEL=\"final_checkpoint_Qwen2.5_dpo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('final_checkpoint_Qwen2.5_dpo/tokenizer_config.json',\n",
       " 'final_checkpoint_Qwen2.5_dpo/special_tokens_map.json',\n",
       " 'final_checkpoint_Qwen2.5_dpo/chat_template.jinja',\n",
       " 'final_checkpoint_Qwen2.5_dpo/vocab.json',\n",
       " 'final_checkpoint_Qwen2.5_dpo/merges.txt',\n",
       " 'final_checkpoint_Qwen2.5_dpo/added_tokens.json',\n",
       " 'final_checkpoint_Qwen2.5_dpo/tokenizer.json')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.model.save_pretrained(PATH_MODEL)\n",
    "tokenizer.save_pretrained(PATH_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model = PeftModel.from_pretrained(base_model, PATH_MODEL)\n",
    "ft_model = ft_model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e85921556f0949f0888181326f5d0c1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c719932ac6d6432ab2e9719edcde2f70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12e2da4c7d7b4721ba7842807510405e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9700f512a2746958adec4d5058610d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "559d1a919cdf43938aaf60131331eb12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/malihoseini/Qwen2.5_dpo/commit/2b8d99540dba61a102609c17cc1b05c067743235', commit_message='Upload tokenizer', commit_description='', oid='2b8d99540dba61a102609c17cc1b05c067743235', pr_url=None, repo_url=RepoUrl('https://huggingface.co/malihoseini/Qwen2.5_dpo', endpoint='https://huggingface.co', repo_type='model', repo_id='malihoseini/Qwen2.5_dpo'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_token = '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'\n",
    "model_name_reop = 'malihoseini/Qwen2.5_dpo'\n",
    "ft_model.push_to_hub(model_name_reop,\n",
    "                  private=True,\n",
    "                  use_temp_dir=True,\n",
    "                  token=hf_token)\n",
    "tokenizer.push_to_hub(model_name_reop,\n",
    "                      private=True,\n",
    "                      use_temp_dir=True,\n",
    "                      token=hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e41f069a058458aa9f5dd32256ed7b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "489e52651c674f1ea877329bea198fad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.09G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecd6108d4d3f4fddb98f2ff128076a04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/242 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "new_model=\"malihoseini/Qwen2.5_dpo\"\n",
    "tokenizer_new_model = AutoTokenizer.from_pretrained(new_model)\n",
    "new_model = AutoModelForCausalLM.from_pretrained(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rejected is : Mexico and the United States have signed an agreement to strengthen security at their shared border, according to the Mexican Public Security Ministry.\n",
      "chosen is :  Sure! Here's a very short summary of the text:\n",
      "\n",
      "Mexico and the United States have signed an agreement to strengthen security at their shared border.\n",
      "model generated output is : A very short summary of the given text is:\n",
      "Mexico-US border security pact announced.\n",
      "This concise statement captures the key elements of the original longer passage, which details Mexico's Mexican Public Security Ministry confirming that the two countries have reached an agreement to strengthen their joint border protection efforts on Thursday.\n"
     ]
    }
   ],
   "source": [
    "print('rejected is :',train_dataset[1]['chosen'])\n",
    "print('chosen is :',train_dataset[1]['rejected'])\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "new_model.to(device)\n",
    "prompt = train_dataset[1]['prompt']\n",
    "inputs = tokenizer_new_model(prompt, return_tensors=\"pt\").to(device)\n",
    "input_token_length = inputs[\"input_ids\"].shape[1]\n",
    "output = tokenizer_new_model.decode(\n",
    "    new_model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=100,\n",
    "    )[0][input_token_length:],\n",
    "    skip_special_tokens=True\n",
    ")\n",
    "print('model generated output is :', output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPuXOqKIFHe/zmE+FCZvu1v",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
